{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_fd52Y8Kkxwg",
        "outputId": "db59b205-1c1e-4aff-dd27-6fd4d062a357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from alphazero_model.pth\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 1 finished: Loss: 76.0598, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 2 finished: Loss: 113.2016, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 3 finished: Loss: 108.2142, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 4 finished: Loss: 101.7542, Phase: movement\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "winner:-1\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 5 finished: Loss: 78.6919, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 6 finished: Loss: 97.3531, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 7 finished: Loss: 81.2854, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 8 finished: Loss: 96.1777, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 9 finished: Loss: 94.3629, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 10 finished: Loss: 90.6779, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 11 finished: Loss: 81.0251, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 12 finished: Loss: 84.1502, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 13 finished: Loss: 79.3556, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 14 finished: Loss: 58.0004, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 15 finished: Loss: 82.7152, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 16 finished: Loss: 87.4849, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 17 finished: Loss: 66.7751, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 18 finished: Loss: 79.7316, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 19 finished: Loss: 75.9535, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 20 finished: Loss: 69.0615, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 21 finished: Loss: 84.7935, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 22 finished: Loss: 67.2145, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 23 finished: Loss: 79.0769, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 24 finished: Loss: 70.1386, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 25 finished: Loss: 75.2138, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 26 finished: Loss: 63.9618, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 27 finished: Loss: 79.4324, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 28 finished: Loss: 63.6194, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 29 finished: Loss: 73.9543, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 30 finished: Loss: 83.1811, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 31 finished: Loss: 79.4144, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 32 finished: Loss: 82.6906, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 33 finished: Loss: 87.8300, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 34 finished: Loss: 88.3074, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 35 finished: Loss: 63.0253, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 36 finished: Loss: 53.6837, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 37 finished: Loss: 78.8870, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 38 finished: Loss: 71.7599, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 39 finished: Loss: 56.1365, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 40 finished: Loss: 91.9077, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 41 finished: Loss: 72.4029, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 42 finished: Loss: 75.0143, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 43 finished: Loss: 81.4118, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 44 finished: Loss: 90.9533, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 45 finished: Loss: 78.8697, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 46 finished: Loss: 74.4525, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 47 finished: Loss: 81.2932, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 48 finished: Loss: 69.5020, Phase: movement\n",
            "Episode 49 finished: Loss: 87.1923, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 50 finished: Loss: 87.5140, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 51 finished: Loss: 80.0685, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 52 finished: Loss: 72.4877, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 53 finished: Loss: 55.3615, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 54 finished: Loss: 77.9133, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 55 finished: Loss: 77.7210, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 56 finished: Loss: 79.9090, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 57 finished: Loss: 85.2907, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 58 finished: Loss: 79.7652, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 59 finished: Loss: 77.3205, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 60 finished: Loss: 89.6609, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 61 finished: Loss: 83.4460, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 62 finished: Loss: 71.4378, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 63 finished: Loss: 82.8486, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 64 finished: Loss: 71.9313, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 65 finished: Loss: 61.9779, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 66 finished: Loss: 72.3198, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 67 finished: Loss: 77.4433, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 68 finished: Loss: 79.9939, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 69 finished: Loss: 75.8187, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 70 finished: Loss: 83.4906, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 71 finished: Loss: 70.5781, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 72 finished: Loss: 56.9724, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 73 finished: Loss: 61.4623, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 74 finished: Loss: 74.1646, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 75 finished: Loss: 104.7984, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 76 finished: Loss: 66.2462, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 77 finished: Loss: 58.3460, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 78 finished: Loss: 84.6911, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 79 finished: Loss: 66.3289, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 80 finished: Loss: 63.7400, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 81 finished: Loss: 93.1545, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 82 finished: Loss: 82.8046, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 83 finished: Loss: 81.3166, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 84 finished: Loss: 87.7552, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 85 finished: Loss: 84.4353, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 86 finished: Loss: 85.9164, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 87 finished: Loss: 80.7772, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 88 finished: Loss: 76.7045, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 89 finished: Loss: 80.9471, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 90 finished: Loss: 90.7280, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 91 finished: Loss: 80.5950, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 92 finished: Loss: 84.3915, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 93 finished: Loss: 64.4035, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 94 finished: Loss: 68.1919, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 95 finished: Loss: 78.8571, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 96 finished: Loss: 73.9377, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 97 finished: Loss: 92.5458, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 98 finished: Loss: 78.2176, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 99 finished: Loss: 86.7137, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 100 finished: Loss: 66.5144, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 101 finished: Loss: 68.5903, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 102 finished: Loss: 85.8939, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 103 finished: Loss: 90.0539, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 104 finished: Loss: 86.4107, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 105 finished: Loss: 89.5978, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 106 finished: Loss: 96.5129, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 107 finished: Loss: 85.8287, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 108 finished: Loss: 65.3058, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 109 finished: Loss: 68.7629, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 110 finished: Loss: 62.1715, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 111 finished: Loss: 82.2921, Phase: movement\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "winner:1\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 112 finished: Loss: 79.6778, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 113 finished: Loss: 70.3377, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 114 finished: Loss: 69.3083, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 115 finished: Loss: 83.0187, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 116 finished: Loss: 90.7791, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 117 finished: Loss: 80.5589, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 118 finished: Loss: 77.8475, Phase: movement\n",
            "Episode 119 finished: Loss: 71.9277, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 120 finished: Loss: 88.9076, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 121 finished: Loss: 90.1369, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 122 finished: Loss: 79.6827, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 123 finished: Loss: 73.3266, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 124 finished: Loss: 76.4687, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 125 finished: Loss: 93.4040, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 126 finished: Loss: 84.5519, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 127 finished: Loss: 51.8640, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 128 finished: Loss: 86.5059, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 129 finished: Loss: 77.7379, Phase: movement\n",
            "Model saved to alphazero_model.pth\n",
            "Episode 130 finished: Loss: 86.8208, Phase: movement\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "from queue import Queue\n",
        "import AlphaExitNet\n",
        "\n",
        "model = 'cuda' if torch.cuda.is_available() else 'mps'\n",
        "\n",
        "class GPUOptimizedAlphaTrainingApp:\n",
        "    def __init__(self, batch_size=128, training_step_delay=1):\n",
        "        self.training_step_delay = training_step_delay\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # GPU 관련 설정\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
        "        if self.device.type == \"cuda\":\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        # 환경과 네트워크 초기화\n",
        "        self.env = AlphaExitNet.ExitStrategyEnv()\n",
        "        self.network = AlphaExitNet.AlphaZeroNet(\n",
        "            board_size=7,\n",
        "            in_channels=2,\n",
        "            num_res_blocks=3,\n",
        "            num_filters=64\n",
        "        )\n",
        "        self.network.to(self.device)\n",
        "        if os.path.exists(\"alphazero_model.pth\"):\n",
        "            AlphaExitNet.load_model(self.network, \"alphazero_model.pth\", self.device)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=2e-4)\n",
        "\n",
        "        # 학습 데이터 관련 변수\n",
        "        self.replay_buffer = []\n",
        "        self.episode_data = []\n",
        "        self.episode_count = 0\n",
        "        self.max_replay_buffer_size = 10000\n",
        "\n",
        "        # MCTS 관련 설정\n",
        "        self.num_simulations = 100\n",
        "        self.temperature = 1.0\n",
        "\n",
        "        # 상태 업데이트를 위한 큐 (터미널 출력용)\n",
        "        self.state_queue = Queue()\n",
        "\n",
        "        # 초기 상태 설정\n",
        "        self.current_state = self.env.reset()\n",
        "        self.start_training_thread()\n",
        "        self.start_terminal_output_thread()\n",
        "\n",
        "    def terminal_output_loop(self):\n",
        "        \"\"\"에피소드가 종료될 때마다 최종 상태를 터미널에 출력\"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                while not self.state_queue.empty():\n",
        "                    msg = self.state_queue.get_nowait()\n",
        "                    # msg는 딕셔너리 형태로 에피소드 종료 정보를 담고 있음\n",
        "                    if msg.get(\"type\") == \"episode_end\":\n",
        "                        ep = msg.get(\"episode_count\")\n",
        "                        loss = msg.get(\"loss\")\n",
        "                        phase = msg.get(\"phase\")\n",
        "                        loss_str = f\"{loss:.4f}\" if loss is not None else \"N/A\"\n",
        "                        print(f\"Episode {ep} finished: Loss: {loss_str}, Phase: {phase}\")\n",
        "            except Exception as e:\n",
        "                print(\"Terminal output error:\", e)\n",
        "            time.sleep(self.training_step_delay)\n",
        "\n",
        "    def start_terminal_output_thread(self):\n",
        "        output_thread = threading.Thread(target=self.terminal_output_loop, daemon=True)\n",
        "        output_thread.start()\n",
        "\n",
        "    def run_mcts(self, state):\n",
        "        \"\"\"GPU에 최적화된 MCTS 실행\"\"\"\n",
        "        legal_moves_mask = AlphaExitNet.get_legal_moves_mask(state, self.env)\n",
        "        with torch.no_grad():\n",
        "            initial_policy, _ = AlphaExitNet.neural_net_fn(state, self.network, self.device, legal_moves_mask)\n",
        "\n",
        "        root_node = AlphaExitNet.Node(state, prior=1.0)\n",
        "        root_node.expand(initial_policy, AlphaExitNet.next_state_func)\n",
        "\n",
        "        action_probs = AlphaExitNet.mcts_search(\n",
        "            root_node,\n",
        "            lambda s: AlphaExitNet.neural_net_fn(s, self.network, self.device, AlphaExitNet.get_legal_moves_mask(s, self.env)),\n",
        "            num_simulations=self.num_simulations,\n",
        "            c_puct=1.0,\n",
        "            next_state_func=AlphaExitNet.next_state_func,\n",
        "            is_terminal_func=AlphaExitNet.is_terminal_func\n",
        "        )\n",
        "        return action_probs\n",
        "\n",
        "    def train_network(self, batch):\n",
        "        \"\"\"GPU에 최적화된 네트워크 학습\"\"\"\n",
        "        total_loss = 0\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Placement phase\n",
        "        placement_examples = [ex for ex in batch if ex[0][\"phase\"] == \"placement\"]\n",
        "        if placement_examples:\n",
        "            states, policies, outcomes = zip(*placement_examples)\n",
        "            state_tensors = torch.stack([\n",
        "                torch.tensor(s['board'], dtype=torch.float32)\n",
        "                for s in states\n",
        "            ]).to(self.device)\n",
        "            policy_tensors = torch.stack([\n",
        "                torch.tensor([policies[i].get(a, 0.0) for a in range(49)], dtype=torch.float32)\n",
        "                for i in range(len(placement_examples))\n",
        "            ]).to(self.device)\n",
        "            outcome_tensors = torch.tensor(outcomes, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "            log_policy, predicted_value = self.network(state_tensors, phase=\"placement\")\n",
        "            loss = AlphaExitNet.compute_loss(log_policy, predicted_value, policy_tensors, outcome_tensors, self.network)\n",
        "            loss.backward()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Movement phase\n",
        "        movement_examples = [ex for ex in batch if ex[0][\"phase\"] == \"movement\"]\n",
        "        if movement_examples:\n",
        "            states, policies, outcomes = zip(*movement_examples)\n",
        "            state_tensors = torch.stack([\n",
        "                torch.tensor(s['board'], dtype=torch.float32)\n",
        "                for s in states\n",
        "            ]).to(self.device)\n",
        "            policy_tensors = torch.stack([\n",
        "                torch.tensor([policies[i].get(a, 0.0) for a in range(24)], dtype=torch.float32)\n",
        "                for i in range(len(movement_examples))\n",
        "            ]).to(self.device)\n",
        "            outcome_tensors = torch.tensor(outcomes, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "            log_policy, predicted_value = self.network(state_tensors, phase=\"movement\")\n",
        "            loss = AlphaExitNet.compute_loss(log_policy, predicted_value, policy_tensors, outcome_tensors, self.network)\n",
        "            loss.backward()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        self.optimizer.step()\n",
        "        return total_loss\n",
        "\n",
        "    def training_loop(self):\n",
        "        \"\"\"별도 스레드에서 실행되는 학습 루프\"\"\"\n",
        "        while True:\n",
        "            state = self.env.reset()\n",
        "            self.episode_data = []\n",
        "\n",
        "            while True:\n",
        "                action_probs = self.run_mcts(state)\n",
        "\n",
        "                # 행동 선택 (temperature 적용)\n",
        "                actions = list(action_probs.keys())\n",
        "                probs = np.array([action_probs[a] for a in actions])\n",
        "                if self.episode_count < 500:  # 초기에는 더 많은 탐험\n",
        "                    probs = probs ** (1 / self.temperature)\n",
        "                probs = probs / np.sum(probs)\n",
        "                action = np.random.choice(actions, p=probs)\n",
        "\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                self.episode_data.append((\n",
        "                    copy.deepcopy(state),\n",
        "                    action_probs,\n",
        "                    state[\"current_player\"],\n",
        "                    reward,\n",
        "                    info\n",
        "                ))\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "                state = next_state\n",
        "\n",
        "            # 에피소드 종료 후 처리\n",
        "            self.episode_count += 1\n",
        "            self.process_episode()\n",
        "\n",
        "            # 모델 저장\n",
        "            if self.episode_count % 1 == 0:\n",
        "                AlphaExitNet.save_model(self.network, \"alphazero_model.pth\")\n",
        "\n",
        "            # Temperature 조정\n",
        "            if self.episode_count % 100 == 0:\n",
        "                self.temperature = max(0.1, self.temperature * 0.95)\n",
        "\n",
        "    def process_episode(self):\n",
        "        \"\"\"에피소드 데이터 처리 및 학습\"\"\"\n",
        "        cumulative_return = 0.0\n",
        "        gamma = 1.0\n",
        "        penalty = -1.0\n",
        "\n",
        "        for (s, mcts_policy, player, r, info) in reversed(self.episode_data):\n",
        "            if info.get(\"max_turn_penalty\", False):\n",
        "                cumulative_return = r + penalty + gamma * cumulative_return\n",
        "            else:\n",
        "                cumulative_return = r + gamma * cumulative_return\n",
        "            self.replay_buffer.insert(0, (s, mcts_policy, cumulative_return))\n",
        "\n",
        "        # 버퍼 크기 제한\n",
        "        if len(self.replay_buffer) > self.max_replay_buffer_size:\n",
        "            self.replay_buffer = self.replay_buffer[-self.max_replay_buffer_size:]\n",
        "\n",
        "        # 배치 학습 및 에피소드 종료 출력\n",
        "        if len(self.replay_buffer) >= self.batch_size:\n",
        "            batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "            loss = self.train_network(batch)\n",
        "            # 에피소드가 끝났을 때만 출력하도록 메시지 큐에 삽입\n",
        "            self.state_queue.put({\n",
        "                \"type\": \"episode_end\",\n",
        "                \"episode_count\": self.episode_count,\n",
        "                \"loss\": loss,\n",
        "                \"phase\": self.env.phase\n",
        "            })\n",
        "\n",
        "    def start_training_thread(self):\n",
        "        training_thread = threading.Thread(target=self.training_loop, daemon=True)\n",
        "        training_thread.start()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = GPUOptimizedAlphaTrainingApp()\n",
        "    while True:\n",
        "        time.sleep(1)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
